{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-projekt (DP): Co zmienia optymalną politykę i wartość startu?\n",
    "\n",
    "## Rozwiązanie\n",
    "\n",
    "### Kontekst\n",
    "W rozdziale 4 zrobiliśmy Dynamic Programming dla tablicowych MDP:\n",
    "- mamy **model** środowiska w postaci `P[s][a] = [(p, s2, r, terminated), ...]`,\n",
    "- umiemy liczyć: `v_pi` (policy evaluation), poprawiać politykę (policy improvement),\n",
    "- znajdować optimum: `pi*`, `v*` (policy iteration / value iteration).\n",
    "\n",
    "### Wybrane pytania badawcze\n",
    "- **Pytanie A**: Wpływ gamma na v*(start)\n",
    "- **Pytanie B**: Deterministyczne vs slippery (FrozenLake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Krok 1: Import bibliotek i funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pretty_matrix_as_grid(v, nrow, ncol, decimals=1):\n",
    "    grid = np.asarray(v, dtype=float).reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "def action_arrows(pi_det, nrow, ncol, arrows):\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r * ncol + c\n",
    "            a = int(pi_det[s])\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))\n",
    "\n",
    "def one_hot_policy(pi_det, nA):\n",
    "    nS = pi_det.shape[0]\n",
    "    pi = np.zeros((nS, nA), dtype=float)\n",
    "    pi[np.arange(nS), pi_det.astype(int)] = 1.0\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Krok 2: Budowanie srodowisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gridworld_AB_P():\n",
    "    nrow, ncol = 5, 5\n",
    "    nS, nA = nrow * ncol, 4\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    A = (0, 1); Aprime = (4, 1); reward_A = 10.0\n",
    "    B = (0, 3); Bprime = (2, 3); reward_B = 5.0\n",
    "    def s2pos(s): return (s // ncol, s % ncol)\n",
    "    def pos2s(r, c): return r * ncol + c\n",
    "    moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "    for s in range(nS):\n",
    "        r, c = s2pos(s)\n",
    "        if (r, c) == A:\n",
    "            s2 = pos2s(*Aprime)\n",
    "            for a in range(nA): P[s][a] = [(1.0, s2, reward_A, False)]\n",
    "            continue\n",
    "        if (r, c) == B:\n",
    "            s2 = pos2s(*Bprime)\n",
    "            for a in range(nA): P[s][a] = [(1.0, s2, reward_B, False)]\n",
    "            continue\n",
    "        for a in range(nA):\n",
    "            dr, dc = moves[a]\n",
    "            r2, c2 = r + dr, c + dc\n",
    "            if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol):\n",
    "                s2, reward = s, -1.0\n",
    "            else:\n",
    "                s2, reward = pos2s(r2, c2), 0.0\n",
    "            P[s][a] = [(1.0, s2, reward, False)]\n",
    "    return P, nS, nA, nrow, ncol\n",
    "\n",
    "def build_frozenlake_P(desc, is_slippery=False):\n",
    "    desc = np.asarray([list(row) for row in desc], dtype='<U1')\n",
    "    nrow, ncol = desc.shape\n",
    "    nS, nA = nrow * ncol, 4\n",
    "    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "    moves = {LEFT: (0, -1), DOWN: (1, 0), RIGHT: (0, 1), UP: (-1, 0)}\n",
    "    def to_s(r, c): return r * ncol + c\n",
    "    def step_from(r, c, a):\n",
    "        dr, dc = moves[a]\n",
    "        r2, c2 = r + dr, c + dc\n",
    "        if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol): r2, c2 = r, c\n",
    "        return r2, c2\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    for r in range(nrow):\n",
    "        for c in range(ncol):\n",
    "            s = to_s(r, c)\n",
    "            tile = desc[r, c]\n",
    "            if tile in ('H', 'G'):\n",
    "                for a in range(nA): P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue\n",
    "            for a in range(nA):\n",
    "                outcomes = []\n",
    "                if is_slippery:\n",
    "                    candidates = [(a - 1) % 4, a, (a + 1) % 4]\n",
    "                    probs = [1/3, 1/3, 1/3]\n",
    "                else:\n",
    "                    candidates, probs = [a], [1.0]\n",
    "                for a_real, p in zip(candidates, probs):\n",
    "                    r2, c2 = step_from(r, c, a_real)\n",
    "                    s2 = to_s(r2, c2)\n",
    "                    tile2 = desc[r2, c2]\n",
    "                    terminated = tile2 in ('H', 'G')\n",
    "                    reward = 1.0 if tile2 == 'G' else 0.0\n",
    "                    outcomes.append((float(p), int(s2), float(reward), bool(terminated)))\n",
    "                merged = {}\n",
    "                for p, s2, rwd, term in outcomes:\n",
    "                    key = (s2, rwd, term)\n",
    "                    merged[key] = merged.get(key, 0.0) + p\n",
    "                P[s][a] = [(p, s2, rwd, term) for (s2, rwd, term), p in merged.items()]\n",
    "    return P, nS, nA, nrow, ncol, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Krok 3: Algorytmy DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(P, pi, gamma=0.9, theta=1e-10, max_iters=100000):\n",
    "    nS, nA = pi.shape\n",
    "    v = np.zeros(nS, dtype=float)\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(nS):\n",
    "            v_old = v[s]\n",
    "            v_new = 0.0\n",
    "            for a in range(nA):\n",
    "                pi_a = pi[s, a]\n",
    "                if pi_a == 0.0: continue\n",
    "                for (p, s2, r, terminated) in P[s][a]:\n",
    "                    if terminated: v_new += pi_a * p * r\n",
    "                    else: v_new += pi_a * p * (r + gamma * v[s2])\n",
    "            v[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "        if delta < theta: break\n",
    "    return v\n",
    "\n",
    "def greedy_policy_from_v(P, v, gamma=0.9):\n",
    "    nS, nA = len(P), len(P[0])\n",
    "    pi_det = np.zeros(nS, dtype=int)\n",
    "    for s in range(nS):\n",
    "        q_values = np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for (p, s2, r, terminated) in P[s][a]:\n",
    "                if terminated: q += p * r\n",
    "                else: q += p * (r + gamma * v[s2])\n",
    "            q_values[a] = q\n",
    "        pi_det[s] = np.argmax(q_values)\n",
    "    return pi_det\n",
    "\n",
    "def policy_iteration(P, gamma=0.9, theta=1e-10, max_iters=1000):\n",
    "    nS, nA = len(P), len(P[0])\n",
    "    pi = np.ones((nS, nA), dtype=float) / nA\n",
    "    for _ in range(max_iters):\n",
    "        v = iterative_policy_evaluation(P, pi, gamma=gamma, theta=theta)\n",
    "        pi_det_new = greedy_policy_from_v(P, v, gamma=gamma)\n",
    "        pi_new = one_hot_policy(pi_det_new, nA)\n",
    "        if np.array_equal(pi, pi_new): break\n",
    "        pi = pi_new\n",
    "    return pi_det_new, v\n",
    "\n",
    "def value_iteration(P, gamma=0.9, theta=1e-10, max_iters=100000):\n",
    "    nS, nA = len(P), len(P[0])\n",
    "    v = np.zeros(nS, dtype=float)\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(nS):\n",
    "            v_old = v[s]\n",
    "            q_values = np.zeros(nA)\n",
    "            for a in range(nA):\n",
    "                q = 0.0\n",
    "                for (p, s2, r, terminated) in P[s][a]:\n",
    "                    if terminated: q += p * r\n",
    "                    else: q += p * (r + gamma * v[s2])\n",
    "                q_values[a] = q\n",
    "            v[s] = np.max(q_values)\n",
    "            delta = max(delta, abs(v_old - v[s]))\n",
    "        if delta < theta: break\n",
    "    pi_det = greedy_policy_from_v(P, v, gamma=gamma)\n",
    "    return pi_det, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Krok 4: Inicjalizacja srodowisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_gw, nS_gw, nA_gw, nrow_gw, ncol_gw = build_gridworld_AB_P()\n",
    "desc4 = ['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
    "P_fl_det, nS_fl, nA_fl, nrow_fl, ncol_fl, desc_fl = build_frozenlake_P(desc4, is_slippery=False)\n",
    "P_fl_slip, _, _, _, _, _ = build_frozenlake_P(desc4, is_slippery=True)\n",
    "\n",
    "arrows_gw = {0:'up', 1:'right', 2:'down', 3:'left'}\n",
    "arrows_fl = {0:'left', 1:'down', 2:'right', 3:'up'}\n",
    "\n",
    "print('Srodowiska zbudowane!')\n",
    "print(f'Gridworld: {nS_gw} stanow, {nA_gw} akcji')\n",
    "print(f'FrozenLake: {nS_fl} stanow, {nA_fl} akcji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Krok 5: Punkt odniesienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_base = 0.99\n",
    "start_state = 0\n",
    "\n",
    "pi_fl_opt, v_fl_opt = value_iteration(P_fl_det, gamma=gamma_base)\n",
    "print(f'FrozenLake (det.), gamma={gamma_base}: v*(start) = {v_fl_opt[start_state]:.6f}')\n",
    "\n",
    "pi_fl_slip_opt, v_fl_slip_opt = value_iteration(P_fl_slip, gamma=gamma_base)\n",
    "print(f'FrozenLake (slippery), gamma={gamma_base}: v*(start) = {v_fl_slip_opt[start_state]:.6f}')\n",
    "\n",
    "pi_gw_opt, v_gw_opt = value_iteration(P_gw, gamma=gamma_base)\n",
    "print(f'Gridworld, gamma={gamma_base}: v*(start) = {v_gw_opt[start_state]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pytanie A: Wplyw gamma na v*(start)\n",
    "\n",
    "**Hipoteza**: Wieksze gamma zwieksza znaczenie przyszlosci i zmienia v*(start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 0.9, 0.95, 0.99, 0.999]\n",
    "\n",
    "vstarts_fl_det = [value_iteration(P_fl_det, gamma=g)[1][start_state] for g in gammas]\n",
    "vstarts_fl_slip = [value_iteration(P_fl_slip, gamma=g)[1][start_state] for g in gammas]\n",
    "vstarts_gw = [value_iteration(P_gw, gamma=g)[1][start_state] for g in gammas]\n",
    "\n",
    "print('Wyniki dla roznych gamma:')\n",
    "print(f\"{'gamma':<10} {'FL det':<12} {'FL slip':<12} {'Gridworld':<12}\")\n",
    "print('-' * 48)\n",
    "for i, g in enumerate(gammas):\n",
    "    print(f'{g:<10.3f} {vstarts_fl_det[i]:<12.4f} {vstarts_fl_slip[i]:<12.4f} {vstarts_gw[i]:<12.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gammas, vstarts_fl_det, 'o-', label='FrozenLake (det)')\n",
    "plt.plot(gammas, vstarts_fl_slip, 's-', label='FrozenLake (slippery)')\n",
    "plt.xlabel('gamma'); plt.ylabel('v*(start)')\n",
    "plt.title('Pytanie A: Wplyw gamma - FrozenLake')\n",
    "plt.grid(True); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gammas, vstarts_gw, 'go-', label='Gridworld')\n",
    "plt.xlabel('gamma'); plt.ylabel('v*(start)')\n",
    "plt.title('Pytanie A: Wplyw gamma - Gridworld')\n",
    "plt.grid(True); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski dla Pytania A\n",
    "\n",
    "Wartosc v*(start) rosnie wraz ze wzrostem gamma w obu srodowiskach. W FrozenLake wzrost jest lagodny (wartosci 0-1), poniewaz jest to zadanie epizodyczne z jedna nagroda +1 na koncu. Dla gamma=0.99 mamy v*(start) okolo 0.95, co odpowiada gamma^L gdzie L to okolo 5 krokow. W Gridworld wartosci rosna wykladniczo (do 2000+ dla gamma=0.999), bo agent zbiera nagrody z teleportow w nieskonczonosc. To pokazuje, ze w zadaniach continuing parametr gamma<1 jest niezbedny dla stabilnosci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pytanie B: Deterministyczne vs Slippery (FrozenLake)\n",
    "\n",
    "**Hipoteza**: Stochastycznosc (slippery=True) obniza v*(start) i zmienia pi*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_comparison = 0.99\n",
    "pi_det, v_det = value_iteration(P_fl_det, gamma=gamma_comparison)\n",
    "pi_slip, v_slip = value_iteration(P_fl_slip, gamma=gamma_comparison)\n",
    "\n",
    "print(f'Dla gamma = {gamma_comparison}:')\n",
    "print(f'  Deterministyczne: v*(start) = {v_det[start_state]:.6f}')\n",
    "print(f'  Slippery:         v*(start) = {v_slip[start_state]:.6f}')\n",
    "print(f'  Spadek: {100*(v_det[start_state]-v_slip[start_state])/v_det[start_state]:.1f}%')\n",
    "\n",
    "print('\\nOptymalna polityka (det.):')\n",
    "action_arrows(pi_det, nrow_fl, ncol_fl, arrows_fl)\n",
    "print('\\nOptymalna polityka (slippery):')\n",
    "action_arrows(pi_slip, nrow_fl, ncol_fl, arrows_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Deterministyczne', 'Slippery'], [v_det[start_state], v_slip[start_state]], \n",
    "        color=['steelblue', 'coral'])\n",
    "plt.ylabel('v*(start)'); plt.title(f'v*(start) dla gamma={gamma_comparison}')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gammas, vstarts_fl_det, 'o-', label='Deterministyczne')\n",
    "plt.plot(gammas, vstarts_fl_slip, 's-', label='Slippery')\n",
    "plt.xlabel('gamma'); plt.ylabel('v*(start)')\n",
    "plt.title('Det. vs Slippery dla roznych gamma')\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski dla Pytania B\n",
    "\n",
    "Stochastycznosc znaczaco obniza v*(start) - z 0.95 do 0.54 (spadek okolo 43%) dla gamma=0.99. Agent w wersji slipery ma mniejsza kontrole nad ruchem, bo kazda akcja jest rozmyta na 3 kierunki (kazdy z p=1/3), co prowadzi do czestszego wpadania w dziury. Zmienia sie tez polityka optymalna: w wersji deterministycznej strzalki prowadza najkrotsza droga, a w slipery polityka unika ruchow przy krawedziach z dziurami, wybierajac bezpieczniejsze sciezki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Podsumowanie\n",
    "\n",
    "**Co zmienialam:** wspolczynnik gamma (0.8-0.999) oraz typ srodowiska (det. vs slippery).\n",
    "\n",
    "**Co zmierzylam:** wartosc stanu startowego v*(start) i ksztalt optymalnej polityki pi*.\n",
    "\n",
    "**Glowne obserwacje:**\n",
    "1. Gamma kontroluje znaczenie przyszlosci - w continuing tasks wartosci rosna dla gamma bliskiego 1.\n",
    "2. Epizodyczne FrozenLake daje v*(start) przyblizenie gamma^L (L = dlugosc sciezki).\n",
    "3. Stochastycznosc obniza wartosci i wymusza ostrozniejsza polityke.\n",
    "4. DP daje dokladne rozwiazanie zarowno dla srodowisk deterministycznych jak i stochastycznych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}